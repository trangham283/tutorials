{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "- ### What is scikit-learn?\n",
    "- ### Quick intro to machine learning\n",
    "- ### General process\n",
    "- ### Walk-through examples\n",
    "    - Classification\n",
    "    - Regression\n",
    "    - (if time) Cross-validation\n",
    "- ### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is sci-kit learn?\n",
    "- scikit-learn.org\n",
    "- \"sci\" = science\n",
    "- a Python module for machine learning\n",
    "- Installation: [instructions for various OS](http://scikit-learn.org/stable/install.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very quick intro to machine learning\n",
    "- Methods of learning (useful) information from data\n",
    "    - Given samples and their attributes (\"features\"), what can we say about some new example? what can we say about the samples in general?\n",
    "    - Examples: weather prediction given previous days' weather data, product recommendation given users' browsing histories, fake news detection given an article/post... and many, many others! \n",
    "- 2 most common problem settings\n",
    "    - Supervised: data available to us come with \"labels\" (i.e. thing we want to predict)\n",
    "        - Classification: prediction of group membership \n",
    "        <img src=\"http://scikit-learn.org/stable/_images/sphx_glr_plot_logistic_multinomial_002.png\", width=500>\n",
    "        - Regression: prediction of a (real) value\n",
    "        <img src=\"http://scikit-learn.org/stable/_images/sphx_glr_plot_isotonic_regression_001.png\", width=500>\n",
    "    - Unsupervised: data available to us don't come with labels\n",
    "        - Clustering: discovery of group membership\n",
    "        <img src=\"http://scikit-learn.org/stable/_images/sphx_glr_plot_dbscan_001.png\", width=500>\n",
    "        - Density estimation: discovery of distribution of data\n",
    "- Today's focus: supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General process (supervised learning)\n",
    "- Training, validation, testing\n",
    "- Training:\n",
    "    - Given: some examples with features and labels\n",
    "    - Learn the relationship between labels and features: usually described by a function **f**, i.e. learning f such that\n",
    "    *label $\\approx$ f(features)*\n",
    "    - How is this *learned*? \n",
    "        - Problem often is formulated as minimizing a cost function (or maximizing a likelihood function)\n",
    "        - Specifically:\n",
    "            - Given data points $(x_1, y_1), ... (x_N, y_N)$ where $x_i$ are usually feature vectors \n",
    "            (i.e. data attributes represented numerically)\n",
    "            - Using your knowledge of probability and what you know about the data, \n",
    "            you hypothesize a possible relationship between $x_i$ and its label $y_i$: \n",
    "            $\\hat{y}_i = f(x_i)$, or commonly: $$\\hat{y}_i = f(\\sum_{j=1}^{m}w_jx_{i,j})$$\n",
    "            The problem then becomes: solving for the weights $w_j$ that helps model this relationship\n",
    "            - Since we have true labels $y_i$, formulate a cost function:\n",
    "            $$L(w) = \\sum_{i=1}^{N}g(y_i, \\hat{y}_i)$$\n",
    "            - Best parameters $w_j$ are then found by solving:\n",
    "            $$w^{\\ast} = argmin_{w} L(w)$$\n",
    "- Testing: \n",
    "    - Given: new data (not seen during training) -- **NEVER** mix training and testing data!\n",
    "    - Predict the labels\n",
    "    - Evaluate predicted labels by comparing to ground truth\n",
    "- Validation:\n",
    "    - Preferably a separate set from training and testing\n",
    "    - Used for tuning parameters of f() and help *prevent overfitting*, aka \"regularization\"\n",
    "    $$L(w) = g(y_i, \\hat{y}_i) + \\lambda||w||^2_2$$\n",
    "    - Sometimes the amount of data is too small --> cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: classification\n",
    "- Problem: detect heart disease from a patient's record\n",
    "- Data: [UCI heart disease data set (short version)](https://archive.ics.uci.edu/ml/datasets/Heart+Disease)\n",
    "    - [train](https://courses.cs.washington.edu/courses/cse546/05sp/psetdata/processed.cleveland.data.train)/[test](https://courses.cs.washington.edu/courses/cse546/05sp/psetdata/processed.cleveland.data.test) -- processed data courtesy of CSE 546 (Spring 2005)\n",
    "    - attributes: 14 (13 features + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# might be necessary, import here first\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# data preprocessing function\n",
    "def load_data(file_name):\n",
    "    flines = open(file_name).readlines()\n",
    "    flines = [x.rstrip() for x in flines]\n",
    "    flines = [x.split(',') for x in flines]\n",
    "    flines = [[float(s) for s in x] for x in flines]\n",
    "    feats = [x[:-1] for x in flines]\n",
    "    labels = [x[-1] for x in flines]\n",
    "    labels = [int(x>0) for x in labels]\n",
    "    return feats, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data downloaded\n",
    "X_train, y_train = load_data('hd.cleveland.data.train.csv')\n",
    "X_test, y_test = load_data('hd.cleveland.data.test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 63.   1.   1. ...,   3.   0.   6.]\n",
      " [ 67.   1.   4. ...,   2.   3.   3.]\n",
      " [ 67.   1.   4. ...,   2.   2.   7.]\n",
      " ..., \n",
      " [ 50.   0.   4. ...,   1.   0.   3.]\n",
      " [ 64.   0.   4. ...,   1.   0.   3.]\n",
      " [ 57.   1.   3. ...,   1.   1.   7.]]\n",
      "(203, 13)\n"
     ]
    }
   ],
   "source": [
    "# peek at X_train, y_train\n",
    "print np.array(X_train)\n",
    "print np.array(X_train).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first try a simple [logistic regression classifier](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\n",
    "- (very abbreviated) math background: \n",
    "    - logistic function: $$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "    ... to model $p(y_i=1 \\,| \\, x_i)$, \n",
    "    and $p(y_i=0 \\, | \\, x_i) = 1 - \\sigma(x_i)$\n",
    "    \n",
    "    - Then decide on predicted label as follows:\n",
    "    $\\hat{y_i} = 1$ if $p(y_i=1 \\,| \\, x_i) > p(y_i=0 \\,| \\, x_i)$ \n",
    "    \n",
    "    and vice versa\n",
    "    \n",
    "    - Recall: $$L(w) = \\sum_{i=1}^{N}g(y_i, \\hat{y}_i)$$\n",
    "    Here \n",
    "    $$-g(y_i, \\hat{y}_i) = y_i\\log\\sigma(x_i) + (1-y_i)\\log(1-\\sigma(x_i))$$\n",
    "    \n",
    "    \n",
    "- Find more [detailed derivation](http://cs229.stanford.edu/notes/cs229-notes1.pdf) in many, many tutorials/texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03986426  0.99840617  0.63739152  0.01867455  0.00452262 -0.52640388\n",
      "   0.42199075 -0.04683685  0.46728745  0.25581275  0.15207522  0.84417809\n",
      "   0.38177672]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "# fit the classifier\n",
    "logistic.fit(X_train, y_train)\n",
    "print logistic.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 1 0 0\n",
      " 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0\n",
      " 0 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0]\n",
      "[0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 0 0\n",
      " 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 0\n",
      " 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# use learned classifier to predict test data y_pred = f(X_test)\n",
    "y_pred = logistic.predict(X_test)\n",
    "print y_pred\n",
    "print np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, evaluating the model:\n",
    "- In classification problems, we often care about\n",
    "    - Precision $p = \\frac{TP}{TP + FP}$\n",
    "    - Recall $r = \\frac{TP}{TP + FN}$\n",
    "    - Accuracy $a = \\frac{TP + TN}{P+N}$\n",
    "    - F1 score $f1 = \\frac{2*r*p}{r+p}$\n",
    "    - CM in sklearn given as \n",
    "    $[[TN,FP];$\n",
    "    $[FN,TP]]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CM:\n",
      "[[48  4]\n",
      " [15 33]]\n",
      "precision: 0.891891891892\n",
      "recall: 0.6875\n",
      "accuracy:  0.81\n",
      "f1:  0.776470588235\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# compute and print scores\n",
    "print \"CM:\"\n",
    "print confusion_matrix(y_test, y_pred)\n",
    "print \"precision:\", precision_score(y_test, y_pred)\n",
    "print \"recall:\", recall_score(y_test, y_pred)\n",
    "print \"accuracy: \", accuracy_score(y_test, y_pred)\n",
    "print \"f1: \", f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to refit the data with some different parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CM:\n",
      "[[48  4]\n",
      " [16 32]]\n",
      "precision: 0.888888888889\n",
      "recall: 0.666666666667\n",
      "accuracy:  0.8\n",
      "f1:  0.761904761905\n"
     ]
    }
   ],
   "source": [
    "logistic = linear_model.LogisticRegression(C=100)\n",
    "logistic.fit(X_train, y_train)\n",
    "y_pred = logistic.predict(X_test)\n",
    "\n",
    "print \"CM:\"\n",
    "print confusion_matrix(y_test, y_pred)\n",
    "print \"precision:\", precision_score(y_test, y_pred)\n",
    "print \"recall:\", recall_score(y_test, y_pred)\n",
    "print \"accuracy: \", accuracy_score(y_test, y_pred)\n",
    "print \"f1: \", f1_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about a different classifier? \n",
    "- There are [lots](http://scikit-learn.org/stable/supervised_learning.html#supervised-learning) to play with: SVM, decision trees are among the most popular default choices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CM:\n",
      "[[51  1]\n",
      " [38 10]]\n",
      "precision: 0.909090909091\n",
      "recall: 0.208333333333\n",
      "accuracy:  0.61\n",
      "f1:  0.338983050847\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(C=100)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print \"CM:\"\n",
    "print confusion_matrix(y_test, y_pred)\n",
    "print \"precision:\", precision_score(y_test, y_pred)\n",
    "print \"recall:\", recall_score(y_test, y_pred)\n",
    "print \"accuracy: \", accuracy_score(y_test, y_pred)\n",
    "print \"f1: \", f1_score(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CM:\n",
      "[[44  8]\n",
      " [19 29]]\n",
      "precision: 0.783783783784\n",
      "recall: 0.604166666667\n",
      "accuracy:  0.73\n",
      "f1:  0.682352941176\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print \"CM:\"\n",
    "print confusion_matrix(y_test, y_pred)\n",
    "print \"precision:\", precision_score(y_test, y_pred)\n",
    "print \"recall:\", recall_score(y_test, y_pred)\n",
    "print \"accuracy: \", accuracy_score(y_test, y_pred)\n",
    "print \"f1: \", f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: regression\n",
    "Same problem setting as above, but predict real-valued scores instead of presence/absence/class membership. \n",
    "We begin with a standard choice: [linear regression](http://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)\n",
    "\n",
    "- Recall: $$L(w) = \\sum_{i=1}^{N}g(y_i, \\hat{y}_i)$$\n",
    "    Here \n",
    "    $$g(y_i, \\hat{y}_i) = (y_i - \\hat{y_i})^2$$\n",
    "\n",
    "    And\n",
    "    $$\\hat{y_i} = \\sum_{j=1}^{m}w_jx_{i,j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply linear regression on our previous heart disease problem\n",
    "- Note/disclaimer: this is just a toy example! \n",
    "- Details on issues/problems later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  2.  1.  0.  0.  0.  3.  0.  2.  1.  0.  0.  2.  0.  0.  0.  1.  0.\n",
      "  0.  0.  0.  0.  1.  3.  4.  0.  0.  0.  0.  3.  0.  2.  1.  0.  0.  0.\n",
      "  3.  1.  3.  0.  4.  0.  0.  0.  1.  4.  0.  4.  0.  0.  0.  0.  2.  0.\n",
      "  1.  1.  1.  1.  0.  0.  2.  0.  1.  0.  2.  2.  1.  0.  2.  1.  0.  3.\n",
      "  1.  1.  1.  0.  1.  0.  0.  3.  0.  0.  0.  3.  0.  0.  0.  0.  0.  0.\n",
      "  0.  3.  0.  0.  0.  1.  2.  3.  0.  0.  0.  0.  0.  0.  3.  0.  2.  1.\n",
      "  2.  3.  1.  1.  0.  2.  2.  0.  0.  0.  3.  2.  3.  4.  0.  3.  1.  0.\n",
      "  3.  3.  0.  0.  0.  0.  0.  0.  0.  0.  4.  3.  1.  0.  0.  1.  0.  1.\n",
      "  0.  1.  4.  0.  0.  0.  0.  0.  0.  4.  3.  1.  1.  1.  2.  0.  0.  4.\n",
      "  0.  0.  0.  0.  0.  0.  1.  0.  3.  0.  1.  0.  4.  1.  0.  1.  0.  0.\n",
      "  3.  2.  0.  0.  1.  0.  0.  2.  1.  2.  0.  3.  1.  2.  0.  3.  0.  0.\n",
      "  0.  1.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing function, version 2 for regression task\n",
    "def load_data_cont(file_name):\n",
    "    flines = open(file_name).readlines()\n",
    "    flines = [x.rstrip() for x in flines]\n",
    "    flines = [x.split(',') for x in flines]\n",
    "    flines = [[float(s) for s in x] for x in flines]\n",
    "    feats = [x[:-1] for x in flines]\n",
    "    labels = [x[-1] for x in flines]\n",
    "    return feats, labels\n",
    "\n",
    "# load data downloaded\n",
    "X_train, y_train = load_data_cont('hd.cleveland.data.train.csv')\n",
    "X_test, y_test = load_data_cont('hd.cleveland.data.test.csv')\n",
    "\n",
    "print np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -6.03270243e-04   1.96276775e-01   2.80906865e-01   6.04260193e-03\n",
      "   2.89013845e-04  -1.57798558e-01   1.37685901e-01  -6.38560647e-03\n",
      "   6.28071200e-02   1.54847785e-01   1.43634231e-01   3.64274504e-01\n",
      "   1.59684804e-01]\n",
      "-1.55286599229\n",
      "MSE:  1.42\n",
      "MAE:  0.76\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.LinearRegression()\n",
    "# fit model:\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# print coefficients\n",
    "print reg.coef_\n",
    "print reg.intercept_\n",
    "\n",
    "# Common evaluation scores for regression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "print \"MSE: \", mean_squared_error(y_test, y_pred)\n",
    "print \"MAE: \", mean_absolute_error(y_test, y_pred) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add simple regularization ([ridge regression](http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -5.97861518e-04   1.95666600e-01   2.80758785e-01   6.03936276e-03\n",
      "   2.88213336e-04  -1.57111316e-01   1.37668124e-01  -6.39156389e-03\n",
      "   6.27571634e-02   1.54926841e-01   1.43273063e-01   3.64008186e-01\n",
      "   1.59771758e-01]\n",
      "-1.55055330931\n",
      "MSE:  1.42\n",
      "MAE:  0.76\n"
     ]
    }
   ],
   "source": [
    "# ridge regressor\n",
    "reg = linear_model.Ridge(alpha=0.1)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# print coefficients\n",
    "print reg.coef_\n",
    "print reg.intercept_\n",
    "\n",
    "# Common evaluation scores for regression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "print \"MSE: \", mean_squared_error(y_test, y_pred)\n",
    "print \"MAE: \", mean_absolute_error(y_test, y_pred) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Play around with others!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.06667992  0.00487795  0.07382952  0.12881689  0.07092588  0.          0.0021317\n",
      "  0.13888155  0.02087771  0.19666514  0.00936427  0.00480968  0.28213979]\n",
      "MSE:  1.42\n",
      "MAE:  0.76\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "reg = DecisionTreeRegressor()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "print reg.feature_importances_\n",
    "\n",
    "# Common evaluation scores for regression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "print \"MSE: \", mean_squared_error(y_test, y_pred)\n",
    "print \"MAE: \", mean_absolute_error(y_test, y_pred) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.76190476  0.70731707  0.475       0.675       0.8       ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf = LinearSVC()\n",
    "\n",
    "# reload classification data\n",
    "X_train, y_train = load_data('hd.cleveland.data.train.csv')\n",
    "print cross_val_score(clf, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important issues to note\n",
    "- Common practice: normalizing features so that values are roughly on the similar scale\n",
    "- Be careful in how you interpret data/what was learned\n",
    "    - In our linear regression example, the problem setting probably does not make a lot of sense\n",
    "- Dealing with real-valued vs categorical data\n",
    "    - categorical data are often represented as one-hot sub-vectors\n",
    "    - real-valued data are often quantized to several levels\n",
    "- Different classifiers are more robust/more appropriate for different problem types -- see [ML cheatsheet](http://scikit-learn.org/stable/tutorial/machine_learning_map/)\n",
    "- Cross validation when there is not enough data -- See [Model selection and Evaluation](http://scikit-learn.org/stable/model_selection.html#model-selection)\n",
    "    - So common and well-accepted that a [stackexchange](https://stats.stackexchange.com/) is named after it\n",
    "    - Divide training data into k partitions (folds) 1, 2, ... k; prefereably keeping the same distribution among folds\n",
    "    - Train k classifiers instead of 1:\n",
    "        - For fold 1, use folds {2, ... k} to train model, then evaluate on data of fold 1\n",
    "        - For fold i, use folds {1, ... k}\\i to train model, then evaluate on data of fold i\n",
    "        - Report scoring metric as average of k scores in each fold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other useful topics to check out\n",
    "- [feature preprocessing/normalization](http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing)\n",
    "- [feature selection](http://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "- [dimensionality reduction](http://scikit-learn.org/stable/modules/decomposition.html#decompositions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- Use [responsibly](https://xkcd.com/643/)\n",
    "<img src=\"https://imgs.xkcd.com/comics/ohm.png\">\n",
    "- Handy guide by sci-kit learn: [ML cheatsheet](http://scikit-learn.org/stable/tutorial/machine_learning_map/)\n",
    "- A very recent critique piece on [misused ML](https://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a)\n",
    "- Try to understand what's going on before implementing/using a certain algorithm\n",
    "- Resources:\n",
    "    - Free Coursera [class(es)](https://www.coursera.org/learn/machine-learning)\n",
    "    - Pedro Domingos' [\"A Few Useful Things to Know about ML\"](http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "    - scikit-learn's [tutorial pages/docs](http://scikit-learn.org/stable/tutorial/index.html)\n",
    "    - Some classic textbooks: \n",
    "        - K. Murphy, Machine Learning: a Probabilistic Perspective, 2012\n",
    "        - C. Bishop, Pattern Recognition and Machine Learning, 2007\n",
    "        - R. Duda, P. Hart & D. Stork, Pattern Classification, 2001\n",
    "        - T. Mitchell, Machine Learning, 1997\n",
    "    - UW classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
